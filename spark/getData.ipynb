{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1번째 반복 중\n",
      "['keyword/keyword_131', 'keyword/keyword_109', 'keyword/keyword_136', 'keyword/keyword_100', 'keyword/keyword_138', 'keyword/keyword_107', 'keyword/keyword_153', 'keyword/keyword_154', 'keyword/keyword_162', 'keyword/keyword_139', 'keyword/keyword_106', 'keyword/keyword_101', 'keyword/keyword_108', 'keyword/keyword_137', 'keyword/keyword_130', 'keyword/keyword_155', 'keyword/keyword_152', 'keyword/keyword_67', 'keyword/keyword_93', 'keyword/keyword_58', 'keyword/keyword_94', 'keyword/keyword_60', 'keyword/keyword_56', 'keyword/keyword_69', 'keyword/keyword_51', 'keyword/keyword_7', 'keyword/keyword_0', 'keyword/keyword_9', 'keyword/keyword_34', 'keyword/keyword_33', 'keyword/keyword_50', 'keyword/keyword_57', 'keyword/keyword_68', 'keyword/keyword_61', 'keyword/keyword_95', 'keyword/keyword_92', 'keyword/keyword_66', 'keyword/keyword_59', 'keyword/keyword_32', 'keyword/keyword_8', 'keyword/keyword_35', 'keyword/keyword_1', 'keyword/keyword_6', 'keyword/keyword_26', 'keyword/keyword_19', 'keyword/keyword_21', 'keyword/keyword_17', 'keyword/keyword_28', 'keyword/keyword_10', 'keyword/keyword_44', 'keyword/keyword_88', 'keyword/keyword_43', 'keyword/keyword_75', 'keyword/keyword_81', 'keyword/keyword_86', 'keyword/keyword_72', 'keyword/keyword_11', 'keyword/keyword_16', 'keyword/keyword_29', 'keyword/keyword_20', 'keyword/keyword_27', 'keyword/keyword_18', 'keyword/keyword_73', 'keyword/keyword_87', 'keyword/keyword_80', 'keyword/keyword_74', 'keyword/keyword_89', 'keyword/keyword_42', 'keyword/keyword_45', 'keyword/keyword_148', 'keyword/keyword_141', 'keyword/keyword_146', 'keyword/keyword_112', 'keyword/keyword_115', 'keyword/keyword_123', 'keyword/keyword_124', 'keyword/keyword_147', 'keyword/keyword_140', 'keyword/keyword_149', 'keyword/keyword_125', 'keyword/keyword_122', 'keyword/keyword_114', 'keyword/keyword_113', 'keyword/keyword_157', 'keyword/keyword_150', 'keyword/keyword_159', 'keyword/keyword_161', 'keyword/keyword_135', 'keyword/keyword_132', 'keyword/keyword_104', 'keyword/keyword_103', 'keyword/keyword_160', 'keyword/keyword_158', 'keyword/keyword_151', 'keyword/keyword_156', 'keyword/keyword_102', 'keyword/keyword_105', 'keyword/keyword_133', 'keyword/keyword_134', 'keyword/keyword_3', 'keyword/keyword_39', 'keyword/keyword_4', 'keyword/keyword_30', 'keyword/keyword_37', 'keyword/keyword_97', 'keyword/keyword_63', 'keyword/keyword_64', 'keyword/keyword_90', 'keyword/keyword_52', 'keyword/keyword_99', 'keyword/keyword_55', 'keyword/keyword_36', 'keyword/keyword_31', 'keyword/keyword_38', 'keyword/keyword_5', 'keyword/keyword_2', 'keyword/keyword_54', 'keyword/keyword_53', 'keyword/keyword_98', 'keyword/keyword_91', 'keyword/keyword_65', 'keyword/keyword_62', 'keyword/keyword_96', 'keyword/keyword_40', 'keyword/keyword_78', 'keyword/keyword_47', 'keyword/keyword_85', 'keyword/keyword_71', 'keyword/keyword_49', 'keyword/keyword_76', 'keyword/keyword_82', 'keyword/keyword_22', 'keyword/keyword_25', 'keyword/keyword_13', 'keyword/keyword_14', 'keyword/keyword_48', 'keyword/keyword_83', 'keyword/keyword_77', 'keyword/keyword_70', 'keyword/keyword_84', 'keyword/keyword_79', 'keyword/keyword_46', 'keyword/keyword_41', 'keyword/keyword_15', 'keyword/keyword_12', 'keyword/keyword_24', 'keyword/keyword_23', 'keyword/keyword_116', 'keyword/keyword_129', 'keyword/keyword_111', 'keyword/keyword_127', 'keyword/keyword_118', 'keyword/keyword_120', 'keyword/keyword_145', 'keyword/keyword_142', 'keyword/keyword_121', 'keyword/keyword_126', 'keyword/keyword_119', 'keyword/keyword_110', 'keyword/keyword_117', 'keyword/keyword_128', 'keyword/keyword_143', 'keyword/keyword_144']\n",
      "163\n",
      "\n",
      "start :: 가열 | 비누 | 어묵 | 담배 | 금연 | 가금 | 전은 | 탄력 | 유와 | 물티슈+\"음식 건강\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd94fe232147465eaf46c5a365257d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb47fe7b34841b6926bd04bac862e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06dfcc84e1414f42bc85a647835ed536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8bd21b5d7db4bada9f6f8b56be4347e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee703c967e04f439c668ef14eb24ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e920c5ada167432f85825080976f853f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5000aff1b8448d8da96dbcc28a55f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3fb566325d24cffb1b600233070909a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9887c8efc4b341d0b8eee5b15272c3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5c8472f564450d8d32ea802c9e7cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027f102f77734977a8bc83226a9770f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125c42485df043f1987c6a156187921e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fcde14feecb4444bf9891c5fd6276c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fd52c315614654b0e8419512b006ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-21ef96e4b04c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"번째 반복 중\"\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mlink_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclient_secret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnaver_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnaver_pw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mget_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4c0f957f3076>\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(client_id, client_secret, naver_id, naver_pw, content_file_name, sc, keywords)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnotebook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnaver_news_link\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "client_id = \"\"\n",
    "client_secret = \"\"\n",
    "naver_id = \"\"\n",
    "naver_pw = \"\"\n",
    "import datetime\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "import glob\n",
    "today = datetime.date.today()\n",
    "content_file_name = \"content_data/naver_news_content_today\".replace(\"today\",str(today))\n",
    "title_file_name = \"temp_data/naver_news_title_today\".replace(\"today\",str(today))\n",
    "sc.stop()\n",
    "sc = SparkContext(master=\"local\", appName=\"spark app\")\n",
    "\n",
    "keywords ='''\"음식 건강\"'''\n",
    "\n",
    "for i in range(30):\n",
    "    print(str(i+1) + \"번째 반복 중\" )\n",
    "    link_count = get_data(client_id, client_secret, naver_id, naver_pw, content_file_name, sc, keywords)\n",
    "    get_topics(link_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(client_id, client_secret, naver_id, naver_pw, content_file_name, sc, keywords):\n",
    "    import os\n",
    "    import sys\n",
    "    import urllib.request\n",
    "    import requests\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from selenium import webdriver\n",
    "    from tqdm import notebook\n",
    "    import pickle\n",
    "    import re\n",
    "    import ast\n",
    "\n",
    "    from bs4 import BeautifulSoup\n",
    "    import urllib\n",
    "    import time\n",
    "\n",
    "    news_data = []\n",
    "    \n",
    "   \n",
    "    page_count = 20\n",
    "    print(glob.glob(\"keyword/keyword*\"))\n",
    "    print(len(glob.glob(\"keyword/keyword*\")))\n",
    "    print()\n",
    "    if len(glob.glob(\"keyword/keyword*\")) > 0 :\n",
    "        keywords_arr = sc.textFile(\"keyword/keyword_\"+str(len(glob.glob(\"keyword/keyword*\"))-1)).collect()\n",
    "        keywords = \"\"\n",
    "        for v in keywords_arr:\n",
    "            keywords += v\n",
    "        keywords += '''+\"음식 건강\"'''\n",
    "    else:\n",
    "        filecount=len(glob.glob(\"keyword/keyword*\"))\n",
    "        sc.parallelize(keywords).saveAsTextFile(\"keyword/keyword_\"+str(filecount))\n",
    "        \n",
    "    print(\"start :: \" + keywords)\n",
    "    encText = urllib.parse.quote(keywords)\n",
    "    \n",
    "    for idx in notebook.tqdm(range(page_count)):\n",
    "        url = \"https://openapi.naver.com/v1/search/news?query=\" + encText + \"&start=\" + str(idx *10 +1)\n",
    "        request = urllib.request.Request(url)\n",
    "        request.add_header(\"X-Naver-Client-Id\",client_id)\n",
    "        request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "        response = urllib.request.urlopen(request)\n",
    "        rescode = response.getcode()\n",
    "\n",
    "        if(rescode==200):\n",
    "            #response_body = response.read()\n",
    "            result = requests.get(response.geturl(),\n",
    "                             headers={\"X-Naver-Client-Id\":client_id,\"X-Naver-Client-Secret\":client_secret})\n",
    "            news_data.append(result.json())\n",
    "        else:\n",
    "            print(\"Error Code:\" + rescode)\n",
    "            break\n",
    "        time.sleep(1)    \n",
    "\n",
    "\n",
    "    naver_news_link = []\n",
    "\n",
    "    for page in news_data:\n",
    "        page_news_link=[]\n",
    "        for item in page['items']:\n",
    "            link = item['link']\n",
    "            #page_news_link.append(link)\n",
    "            if \"naver\" in link:\n",
    "                page_news_link.append(link)\n",
    "        naver_news_link.append(page_news_link)        \n",
    "\n",
    "    link_count = 0        \n",
    "    for page in naver_news_link:\n",
    "         for link in page:\n",
    "                link_count += 1 \n",
    "\n",
    "    naver_news_title = []\n",
    "    naver_news_content = []\n",
    "    \n",
    "    for n in notebook.tqdm(range(len(naver_news_link))):\n",
    "        news_page_title = []\n",
    "        news_page_content = []\n",
    "\n",
    "        if n==0 or n%5 == 0:\n",
    "            chrome_options = webdriver.ChromeOptions()\n",
    "            chrome_options.add_argument('--no-sandbox')\n",
    "            chrome_options.add_argument('--window-size=1420,1080')\n",
    "            chrome_options.add_argument('--headless')\n",
    "            chrome_options.add_argument('--disable-gpu')\n",
    "            chrome_options.add_argument('--disable-dev-shm-usage')        \n",
    "            driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "            driver.get(\"https://nid.naver.com/nidlogin.login\")\n",
    "            driver.find_element_by_name('id').send_keys(naver_id)\n",
    "            driver.find_element_by_name('pw').send_keys(naver_pw)\n",
    "\n",
    "        for idx in notebook.tqdm(range(len(naver_news_link[n]))):\n",
    "            time.sleep(2)\n",
    "\n",
    "            try:\n",
    "\n",
    "                driver.get(naver_news_link[n][idx])\n",
    "            except:\n",
    "                print(\"Timeout!\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                response = driver.page_source\n",
    "\n",
    "            except UnexpectedAlertPresentException:\n",
    "                driver.switch_to_alert().accept()\n",
    "                print(\"게시글이 삭제됨\")\n",
    "                continue\n",
    "\n",
    "            soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "            title = None\n",
    "\n",
    "            try:\n",
    "                item = soup.find('div', class_=\"article_info\")\n",
    "                title = item.find('h3', class_=\"tts_head\").get_text()\n",
    "            except:\n",
    "                title = \"OUTLINK\"\n",
    "\n",
    "            news_page_title.append(title)\n",
    "\n",
    "            doc = None\n",
    "            text= \"\"\n",
    "\n",
    "            data = soup.find_all(\"div\",{\"class\" : \"_article_body_contents\"})\n",
    "            if data:\n",
    "                for item in data:\n",
    "                    text = text + str(item.find_all(text=True)).strip()\n",
    "                    text = ast.literal_eval(text)\n",
    "                    doc = ' '.join(text)\n",
    "            else:\n",
    "                doc = \"OUTLINK\"\n",
    "\n",
    "            news_page_content.append(doc.replace('\\n',' '))\n",
    "\n",
    "        naver_news_title.append(news_page_title)\n",
    "        naver_news_content.append(news_page_content)\n",
    "\n",
    "    filecount=len(glob.glob(\"content_data/*\"))\n",
    "    sc.parallelize(naver_news_content).saveAsTextFile(content_file_name+\"_\"+str(filecount))\n",
    "    \n",
    "    return link_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(link_count):\n",
    "    import string\n",
    "    import re\n",
    "    from konlpy.tag import Mecab\n",
    "    from tqdm import notebook\n",
    "    from gensim import corpora\n",
    "    from gensim import models\n",
    "    import numpy as np\n",
    "    import json\n",
    "\n",
    "    \n",
    "    NUM_TOPICS = link_count \n",
    "    NUM_TOPIC_WORDS = 30 \n",
    "\n",
    "    def build_doc_term_mat(documents):\n",
    "        dictionary = corpora.Dictionary(documents)\n",
    "        corpus = [dictionary.doc2bow(document) for document in documents]\n",
    "        return corpus, dictionary\n",
    "\n",
    "\n",
    "    documents=[]\n",
    "    #for filename in glob.glob(\"content_data/*\"):\n",
    "        #documents += sc.textFile(filename).collect()\n",
    "    \n",
    "    documents += sc.textFile(glob.glob(\"content_data/*\")[(len(glob.glob(\"content_data/*\"))-1)]).collect()    \n",
    "    #documents = read_documents(content_file_name)\n",
    "    \n",
    "    #SW = define_stopwords(\"stopwords-ko.txt\")\n",
    "    SW = set()\n",
    "    for i in string.punctuation:\n",
    "        SW.add(i)\n",
    "\n",
    "    with open(\"stopwords-ko.txt\") as f:\n",
    "        for word in f:\n",
    "            SW.add(word.replace(\"\\n\",\"\"))\n",
    "    \n",
    "    custom_sw = []\n",
    "    if len(glob.glob(\"keyword/keyword*\")) > 0 :\n",
    "        glob.glob(\"keyword/keyword*\")\n",
    "    \n",
    "        for v_ in glob.glob(\"keyword/keyword*\"):\n",
    "            \n",
    "            keywords_arr = sc.textFile(v_).collect()\n",
    "            keywords = \"\"\n",
    "            for v in keywords_arr:\n",
    "                keywords += v\n",
    "\n",
    "            keywords.replace(\"|\", \"\")\n",
    "            keywords = keywords.split()\n",
    "            for v in keywords:\n",
    "                if(v != (\"|\")):\n",
    "                    custom_sw.append(v)\n",
    "\n",
    "    cleaned_text = []\n",
    "    for doc in documents:\n",
    "        temp_doc = re.sub(\"[^ㄱ-ㅎㅏ-ㅣ가-힇 ]\",\"\",doc)\n",
    "        for v in custom_sw:\n",
    "            temp_doc = temp_doc.replace(v,\"\")\n",
    "        cleaned_text.append(temp_doc)\n",
    "    \n",
    "   \n",
    "    \n",
    "    mecab = Mecab()\n",
    "    tokenized_text = []\n",
    "    tokenizer=\"noun\"\n",
    "    if tokenizer == \"noun\":\n",
    "        for n in notebook.tqdm(range(len(cleaned_text)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.nouns(cleaned_text[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            tokenized_text.append(token_text)\n",
    "    elif tokenized == \"morph\":\n",
    "        for n in notebook.tqdm(range(len(cleaned_text)), desc=\"Preprocessing\"):\n",
    "            token_text = mecab.morphs(cleaned_text[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            tokenized_text.append(token_text)\n",
    "    elif tokenized == \"word\":\n",
    "        for n in notebook.tqdm(range(len(cleaned_text)), desc=\"Preprocessing\"):\n",
    "            token_text = cleaned_text[n].split()\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            tokenized_text.append(token_text)\n",
    "    \n",
    "\n",
    "    dictionary = corpora.Dictionary(tokenized_text)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_text]\n",
    "\n",
    "    model = models.ldamodel.LdaModel(corpus, num_topics=link_count, id2word=dictionary)\n",
    "\n",
    "    # document-term matrix\n",
    "    dictionary = corpora.Dictionary(tokenized_text)\n",
    "    corpus = [dictionary.doc2bow(document) for document in tokenized_text]\n",
    "\n",
    "    # LDA 실행\n",
    "    model = models.ldamodel.LdaModel(corpus, num_topics=NUM_TOPICS, id2word=dictionary, alpha=\"auto\", eta=\"auto\")\n",
    "\n",
    "    import pyLDAvis\n",
    "    import pyLDAvis.gensim\n",
    "\n",
    "    pyLDAvis.enable_notebook()\n",
    "\n",
    "    data = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
    "    \n",
    "    filecount=len(glob.glob(\"html_results/*\"))\n",
    "    \n",
    "    pyLDAvis.save_html(data, 'html_results/lda_'+str(filecount)+'.html')\n",
    "\n",
    "    \n",
    "    temp = data.topic_info.head(10)['Term'].values\n",
    "    topic_words =\"\"\n",
    "    for i,v in enumerate(temp):\n",
    "        if i == len(temp)-1:\n",
    "            topic_words += v \n",
    "        else:\n",
    "            topic_words += v + \" | \"\n",
    "    print(\"end :: \" + topic_words)\n",
    "\n",
    "    filecount=len(glob.glob(\"keyword/keyword*\"))\n",
    "    sc.parallelize(topic_words).saveAsTextFile(\"keyword/keyword_\"+str(filecount))\n",
    "    \n",
    "    #data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
